# configs/eole/training.yaml
training:
  world_size: 1
  gpu_ranks: [0]
  batch_size: 32
  batch_type: tokens
  optim: adam
  learning_rate: 2.0
  warmup_steps: 8000
  decay_method: noam
  adam_beta2: 0.998
  dropout: [0.1]
  attention_dropout: [0.1]
  max_grad_norm: 0
  label_smoothing: 0.1
  param_init: 0
  param_init_glorot: true
  normalization: tokens
  model_path: models/pictonmt
  save_checkpoint_steps: 2000
  train_steps: 100000
  valid_steps: 2000
  report_every: 200